{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6b6e12732a196a9010ee6725d62f20956f2a7b6"
   },
   "source": [
    "# Behaviour based content clustering\n\n## Kernel - 3 for DonorsChoose.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83da5b48f6813543aa8ff6c182f2df5057315b3e"
   },
   "source": [
    "In this kernel, we will examine the behavior of people and do the content based filtering in a very intelligible way that it will perform at its best. The main idea behind this kernel will be to examine how they behave is changing from one donation to the next donation and make categories in such a way that it accounts for the gradual change in the behavior of people and give best possible results for a content-based filter.\n\nThe idea behind this kernel is that if the behavior is changing drastically then one can't recommend similar content to last donation. If we are going to suggest similar content we should ensure that the behaviour is not changing ( or not changing across the variables that we are selecting for content suggestion) or we have to keep checking if the behaviour is different every once in a while and should make changes in the variables such that the content suggested by us will be the optimum quality of content a donor is interested in.\n\n### This analysis will have two parts - \n                    - Input prep such that behaviour is same for selected variables\n                    - Clustering based recommendation engine - for suggesting donors for projects\n                    \n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9742797d065ded69572dafde043f7de35211b3fb"
   },
   "source": [
    "## Workflow diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4da6905b68c5eb360fd56068495dacd31881c989"
   },
   "source": [
    "![](https://lh3.googleusercontent.com/-ZvnzeqXmNak/WyoYgBgesTI/AAAAAAAAvcU/cr4r0TFfMEMnbXGRA1q_vBdSK3keIsJZQCL0BGAYYCw/h1183/2018-06-20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": false,
    "_uuid": "116493bc2c9bffcca38945adf3d2a2a46f292e99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing packages \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader, Dataset, SVD, evaluate\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import time\n",
    "import datetime\n",
    "from scipy.sparse.linalg import svds\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "output_notebook()\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.tools as tls\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from functools import reduce\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0797998f4b2f864b77b26ad8f8c42a8336c0044"
   },
   "source": [
    "## Loading Data\nWe will load the data files, and merge the projects file with donations file so that we can have the information that what type of projects gets the attention of what type of donors. We will filter the donors which has donoated twice and divide the datset into two dataframes - dataframe having first donation and dataframe having second donations. Between these two dataframe we will analyse the change in behaviour between first and second donation and will make our hypothesis based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "0a4d2dd82665a8c4a2016612213f661d1db37e7d"
   },
   "outputs": [],
   "source": [
    "donations = pd.read_csv(\"../input/Donations.csv\", encoding='latin-1')\n",
    "donors = pd.read_csv(\"../input/Donors.csv\", encoding='latin-1')\n",
    "projects = pd.read_csv(\"../input/Projects.csv\", encoding='latin-1')\n",
    "resources = pd.read_csv(\"../input/Resources.csv\", encoding='latin-1')\n",
    "schools = pd.read_csv(\"../input/Schools.csv\", encoding='latin-1')\n",
    "teachers = pd.read_csv(\"../input/Teachers.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2769de97621bb76848e805ee5f3e6168bc17af1a"
   },
   "source": [
    "## Input Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": false,
    "_uuid": "97d4a2e63d207e1d41cf7ff1967c66dac38bf7e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the donations count to get the number of donations\n",
    "donations_count = pd.DataFrame(\n",
    "    donations.groupby(\"Donor ID\")['Project ID'].count()\n",
    ")\n",
    "donations_count.reset_index(inplace = True)\n",
    "donations_count.rename(columns = {'Project ID':'donation_count'},\n",
    "                       inplace = True)\n",
    "donations_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "9cb429b00a628824e0e25445b4b63e8533d67873"
   },
   "outputs": [],
   "source": [
    "donors_having_two_donations = \\\n",
    "    donations_count.loc[donations_count['donation_count'] == 2]\n",
    "# Donors having two donations has data of donors who has donated twice\n",
    "\n",
    "data_ = donors_having_two_donations.merge(donations, on='Donor ID',\n",
    "        how='right')\n",
    "# data_ dataframe has donations data for donors having two donations\n",
    "\n",
    "data_.sort_values(['Donor ID', 'Donation Received Date'], inplace=True)\n",
    "# Sort these values so that these two donoations are adjecent to each other\n",
    "\n",
    "\n",
    "odd = [x for x in list(range(0, 274902)) if x%2 ==1] \n",
    "even = [x for x in list(range(0, 274902)) if x%2 ==0] \n",
    "first = data_.loc[even]  # data having first donations of data\n",
    "second = data_.loc[odd]  # data for second donations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "65deea69351d160534185e5e34d16be7aa4a97c8"
   },
   "outputs": [],
   "source": [
    "# merging projects and schools file in first and second donations\n",
    "first_df = projects.merge(first, left_on='Project ID',\n",
    "                          right_on='Project ID', how='right')\n",
    "second_df = projects.merge(second, left_on='Project ID',\n",
    "                           right_on='Project ID', how='right')\n",
    "\n",
    "first_df = first_df.merge(schools, left_on='School ID',\n",
    "                          right_on='School ID', how='left')\n",
    "second_df = second_df.merge(schools, left_on='School ID',\n",
    "                            right_on='School ID', how='left')\n",
    "\n",
    "# Save the copy of first_df and second_df\n",
    "first_df_copy = first_df.copy()\n",
    "second_df_copy = second_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9625142cdccc63e3fcb88330a16f4e4247eb9d9b"
   },
   "source": [
    "## Analysing and Adjusting Behaviour \nFor analyzing behavior, we will have to check for each variable that if a donor who had a preference A in past donation, what is his/her preference for current donation. It can be made clear with two examples\n                - A) Donors that were donated to the rural school in past, are donating X% to rural and rest y% to urban\n                - B) Donor donating to state A is donating to state A only.\nIn these two examples, in example A, the behavior is changing but in example B the behavior is constant. we have to analyze for all the people and see if the significant number of people's behavior is changing then we will merge the two levels of that parameters (adjust that parameters) and that will/may result in making the behavior constant regarding that parameter. If e continue to do so for all the variables we use, we will reach to a point, where across the adjusted parameters, the behavior change is least.\n\n** For examining the behavior change, I will be using simple confusion matrices (heatmaps of past vs current behavior). One can use any other kind of plots like Sankey or alluvial plots. This will also show that a simple plot like confusion matrix can be a basis for making proper input**\n\nIn example shown below,  following variables will be adjusted - \n                    - A) Project Grade Level Category\n                    - B) Project Resource Category\n                    - C) School State\n                    - D) School Metro Type\n                    - E) School Percentage Free Lunch - will be making categories for this\n                    - F) Project Cost - will be making categories for this\n                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "1e5380cf592c843d815cb06dbb7a09e09ed44787"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "        Input - \n",
    "                cm - confusion matrics\n",
    "                classes - classes\n",
    "        Output - \n",
    "                Plots confusion matrix\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    fig = plt.figure(figsize = (14,14))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('Current behaviour')\n",
    "    plt.xlabel('Past behaviour')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "246b4c13cb1150cfe399afe76e935e75e51c86e2"
   },
   "outputs": [],
   "source": [
    "def check_behaviour_shift(var, first_df, second_df):\n",
    "    \"\"\"\n",
    "    Function to check the behaviour shift in the first\n",
    "    donation to second donation\n",
    "        Input - \n",
    "                var - variables to be plotted \n",
    "                first_df - past donations dataframe\n",
    "                second_df - current donations dataframe\n",
    "    \"\"\"\n",
    "    first = [str(x) for x in first_df[var].values]\n",
    "    second = [str(x) for x in second_df[var].values]\n",
    "    label = list(set(first))\n",
    "    print(var)\n",
    "    cnf_matrix = confusion_matrix(first, second, label)\n",
    "    cnf_matrix\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix, label,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Accent)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "fc578adcfcf63bc3e9f97ff89a73a61025c91de5"
   },
   "outputs": [],
   "source": [
    "def variable_adjustment(\n",
    "    first_df,\n",
    "    second_df,\n",
    "    var,\n",
    "    list_values,\n",
    "    ):\n",
    "    \"\"\"Function to merge the distinct values of variable and make them as one\"\"\"\n",
    "\n",
    "    first = first_df.copy()\n",
    "    second = second_df.copy()\n",
    "    new_value = ''\n",
    "    for values in list_values:\n",
    "        new_value = new_value + ' ' + values\n",
    "\n",
    "    # first.loc[(first[var].isin(list_values)), var] = new_value\n",
    "\n",
    "    first[var] = np.where(first[var].isin(list_values), new_value,\n",
    "                          first[var])\n",
    "\n",
    "    # second.loc[(second[var].isin(list_values)), var] = new_value\n",
    "\n",
    "    second[var] = np.where(second[var].isin(list_values), new_value,\n",
    "                           second[var])\n",
    "    return (first, second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e4e189562d7b4de9e02f28f00fb6238a7020bfa"
   },
   "source": [
    "#### 1. Project Grade Level Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": false,
    "_uuid": "4be93c5bccdb685647b447679dea47129ae22159",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift('Project Grade Level Category', first_df, second_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "trusted": false,
    "_uuid": "2c1ccd3f1efd23bf389c0aeb9534340e3d4d00a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first Adjustment\n",
    "list_values = ['Grades PreK-2', 'Grades 6-8']\n",
    "(first_df_1, second_df_1) = variable_adjustment(first_df, second_df,\n",
    "        'Project Grade Level Category', list_values)\n",
    "check_behaviour_shift('Project Grade Level Category', first_df_1,\n",
    "                      second_df_1)\n",
    "\n",
    "# second Adjustment\n",
    "list_values2 = ['Grades 3-5', 'Grades 9-12']\n",
    "(first_df_2, second_df_2) = variable_adjustment(first_df_1,\n",
    "        second_df_1, 'Project Grade Level Category', list_values2)\n",
    "check_behaviour_shift('Project Grade Level Category', first_df_2,\n",
    "                      second_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3297bbf3555cabd21ae9334ccc0d57aceb8e96c0"
   },
   "source": [
    "** Its is clear from the above plots that this variable should not be used for clustering as merging all variables only will result in consistant behaviour**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cec9f625eb30bd80f09185727ec0a2b8a7658aa9"
   },
   "source": [
    "#### 2 Project Resource Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": false,
    "_uuid": "d3e81d463ee3f7a528c62dbd257251afb3befb1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift('Project Resource Category', first_df_copy, second_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": false,
    "_uuid": "75802c8da96716ceec03854d17caccc7b5483804",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# behaviour adjustment \n",
    "list_values3 = ['Others', 'Supplies', 'Books', 'Technology']\n",
    "var = 'Project Resource Category'\n",
    "(first_df_3, second_df_3) = variable_adjustment(first_df, second_df,\n",
    "        var, list_values3)\n",
    "check_behaviour_shift(var, first_df_3, second_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "320a9f013e8b0e08d26156430af12368ffda7997"
   },
   "source": [
    "After adjustment we can see that the category - \"Others Supplies Books Technology\" has most filled and is on diagonal so behaviour is change the least from past to current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac1203f4a96d7503e895d1adac8990d960bcb248"
   },
   "source": [
    "#### 3. School State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": false,
    "_uuid": "40343cec1ec1c5a42787d370f266611a221c232a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift('School State', first_df_copy, second_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": false,
    "_uuid": "f79e68e0b4e2047e0fac22e0eef83761476163ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# behaviour adustment\n",
    "list_values4 = [\n",
    "    'Illinois',\n",
    "    'Florida',\n",
    "    'Hawaii',\n",
    "    'New York',\n",
    "    'California',\n",
    "    'Washington',\n",
    "    'North Carolina',\n",
    "    'Texas',\n",
    "    'Georgia',\n",
    "    ]\n",
    "var = 'School State'\n",
    "(first_df_4, second_df_4) = variable_adjustment(first_df_3,\n",
    "        second_df_3, var, list_values4)\n",
    "check_behaviour_shift(var, first_df_4, second_df_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab99d8394d4b20dd7d87de4ca2857fa662d30066"
   },
   "source": [
    "After adjustment the most occupied entry is on diagonal so behave is consistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad31764c69fe66b75008c8ac11fdaf83a9afc03"
   },
   "source": [
    "#### 4. School Metro Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": false,
    "_uuid": "684eff67648e715ab232c92d3cab8a6c5a9aeba9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift('School Metro Type', first_df_copy, second_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": false,
    "_uuid": "1a65d39703ce6d18e1e252878b7d35dd190a1107",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_values5 = ['urban', 'unknown', 'suburban']\n",
    "var = 'School Metro Type'\n",
    "(first_df_5, second_df_5) = variable_adjustment(first_df_4,\n",
    "        second_df_4, var, list_values5)\n",
    "check_behaviour_shift(var, first_df_5, second_df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": false,
    "_uuid": "0e3dded14446397233b8cae4d23b3561555e0aa1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables to be digitize - 1. School free lunch and 2. project cost\n",
    "fig, ax = plt.subplots(figsize = (18,9),ncols=2, sharex=False, sharey=False)\n",
    "sns.despine()\n",
    "sns.distplot(first_df['School Percentage Free Lunch'].dropna(), ax=ax[0])\n",
    "sns.distplot(np.log(first_df['Project Cost'].dropna()+1), ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ef869d8787c3ae0c0fefceb7911dc497b1cbf19"
   },
   "source": [
    "#### Digitizing % free lunch and project cost\nWe will be using the log of project cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": false,
    "_uuid": "1b8b75a61d59b1a652681644e866086b40edb2b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prec_free_lunch_bins = [\n",
    "    0,\n",
    "    20,\n",
    "    40,\n",
    "    60,\n",
    "    80,\n",
    "    100,\n",
    "    ]\n",
    "first_df_5['perc_freee_lunch'] = \\\n",
    "    np.digitize(first_df_5['School Percentage Free Lunch'].values,\n",
    "                prec_free_lunch_bins)\n",
    "second_df_5['perc_freee_lunch'] = \\\n",
    "    np.digitize(second_df_5['School Percentage Free Lunch'].values,\n",
    "                prec_free_lunch_bins)\n",
    "\n",
    "proj_cost_bins = [\n",
    "    0,\n",
    "    4,\n",
    "    6,\n",
    "    8,\n",
    "    10,\n",
    "    20,\n",
    "    ]\n",
    "first_df_5.loc[:, 'log_proj_cost'] = np.log(first_df_5['Project Cost'\n",
    "        ].dropna() + 1)\n",
    "second_df_5.loc[:, 'log_proj_cost'] = np.log(second_df_5['Project Cost'\n",
    "        ].dropna() + 1)\n",
    "first_df_5['proj_cost'] = np.digitize(first_df_5['log_proj_cost'\n",
    "        ].values, proj_cost_bins)\n",
    "second_df_5['proj_cost'] = np.digitize(second_df_5['log_proj_cost'\n",
    "        ].values, proj_cost_bins)\n",
    "\n",
    "first_df_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54f11486880582e2e2c65090736cfc9f965e1734"
   },
   "source": [
    "#### 5. proj_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": false,
    "_uuid": "2ab39668fd3b8e2e83d7dbedfe1418f6b0678e56",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift('proj_cost', first_df_5, second_df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": false,
    "_uuid": "9d7a9d4ca2bb519e5d2a48e8e0b1908a4beef6a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_values6 = ['2', '3']\n",
    "var = 'proj_cost'\n",
    "(first_df_6, second_df_6) = variable_adjustment(first_df_5,\n",
    "        second_df_5, var, list_values6)\n",
    "check_behaviour_shift(var, first_df_6, second_df_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "771f3a2a9a574cf6b40b3993ca7a12e6b59ee7e3"
   },
   "source": [
    "#### 6. perc_freee_lunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": false,
    "_uuid": "ccd4905314eb4332b03a4783f0e98ddf405b98d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_behaviour_shift(\"perc_freee_lunch\", first_df_6, second_df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": false,
    "_uuid": "84bbc510e8617451f18c3a91d73c328a4da86e56",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_values7 = ['4', '5', '2', '3']\n",
    "var = 'perc_freee_lunch'\n",
    "(first_df_7, second_df_7) = variable_adjustment(first_df_6,\n",
    "        second_df_6, var, list_values7)\n",
    "check_behaviour_shift(var, first_df_7, second_df_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4084038546ab9ad0308c49f91f2e80b3d072eed6"
   },
   "source": [
    "# Content based clustering on adjusted project attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd90f662e8fe727452b87a2c19f480ce32057a4d"
   },
   "source": [
    " ### Create dummy for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": false,
    "_uuid": "d694a23454832f57983fd85bc30f1baabbde8f09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_var = [\n",
    "    'Project Grade Level Category',\n",
    "    'Project Resource Category',\n",
    "    'School State',\n",
    "    'School Metro Type',\n",
    "    'proj_cost',\n",
    "    'perc_freee_lunch',\n",
    "    ]\n",
    "\n",
    "full_data = pd.concat([first_df_7, second_df_7], axis=0)\n",
    "full_data.shape\n",
    "full_data.reset_index(inplace = True)\n",
    "\n",
    "list_7 = first_df_7.columns\n",
    "\n",
    "full_data_2 = pd.get_dummies(data=full_data, columns=list_var)\n",
    "list_8 = full_data_2.columns\n",
    "dummies_col = [x for x in list_8 if x not in list_7]\n",
    "first_df_8 = full_data_2.head(274902 // 2)\n",
    "first_df_8[dummies_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": false,
    "_uuid": "548c8fd07132230a7794a2e7b4bc8f57b3709079",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Assign_cluster(\n",
    "    data1,\n",
    "    data2,\n",
    "    list_var,\n",
    "    clus_size,\n",
    "    ):\n",
    "    \"\"\"Function to assign cluster using k means clustering\n",
    "        Input - \n",
    "                data1/2 - dataframes having adjusted variables\n",
    "                list_var - list of variables for clustering\n",
    "                clus_size - Number of clusters\n",
    "        Output - \n",
    "                data - this data has cluster number \n",
    "                \n",
    "    \"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(data1.shape[0])\n",
    "    data = pd.concat([data1, data2], axis=0)\n",
    "    print(data.shape[0])\n",
    "    kmeans = MiniBatchKMeans(n_clusters=clus_size,\n",
    "                             batch_size=10000).fit(data[list_var])\n",
    "    data1.loc[:, 'cluster'] = kmeans.predict(data1[list_var])\n",
    "    data2.loc[:, 'cluster'] = kmeans.predict(data2[list_var])\n",
    "    t1 = time.time()\n",
    "    print('Time taken in clustering is {}'.format(t1 - t0))\n",
    "    return (data1, data2)\n",
    "\n",
    "\n",
    "(data_1, data_2) = Assign_cluster(first_df_8, full_data_2.loc[137452:],\n",
    "                                  dummies_col, 120)\n",
    "data_1.head()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7abb16cdba8af949f79b3d9fc97805bd82de324"
   },
   "source": [
    "#### Vasualize number of donors per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": false,
    "_uuid": "22049e958d0096a32bc2065d405122f378735433",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking the distribution across clusters\n",
    "plt.hist(data_2['cluster'].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": false,
    "_uuid": "0a3db9374ef3b13fb646f62eea51f6f71bd6f275",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Prediction_on_cluster_based_engine(train_df, test_df, n):\n",
    "    \"\"\"Function to predict using the cosine distnace based engine\"\"\"\n",
    "\n",
    "    test_df['reco_ids'] = ''\n",
    "\n",
    "    def predictions(row, train_df=train_df):\n",
    "        ids = list(train_df.loc[train_df['cluster'] == row['cluster'\n",
    "                   ]]['Donor ID'])[0:n]\n",
    "        return ids\n",
    "\n",
    "    test_df['reco_ids'] = test_df.apply(lambda row: predictions(row),\n",
    "            axis=1)\n",
    "    return test_df\n",
    "\n",
    "\n",
    "test_clus = Prediction_on_cluster_based_engine(data_1,\n",
    "        data_2[5000:5500], 5000)\n",
    "test_clus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "953aecff2efc28f225c5090d647573c762a00688"
   },
   "outputs": [],
   "source": [
    "def cosine_sim_reco(\n",
    "    data,\n",
    "    new_project_profile,\n",
    "    var_list,\n",
    "    dummies_col,\n",
    "    n,\n",
    "    ):\n",
    "    \"\"\"Function to calculate cosine distance between two donors\"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    data_copy = data.copy()\n",
    "    data_copy.reset_index(inplace=True)\n",
    "\n",
    "    def get_project_profile(row, proj=new_project_profile,\n",
    "                            var_list=var_list):\n",
    "        profile = np.array(row[dummies_col])\n",
    "        proj = np.array(proj)\n",
    "        cs = cosine_similarity(profile.reshape(1, -1), proj.reshape(1,\n",
    "                               -1))\n",
    "        return cs[0][0]\n",
    "\n",
    "    data_copy['cosine_'] = data_copy.apply(lambda row: \\\n",
    "            get_project_profile(row), axis=1)\n",
    "    data_copy = data_copy.sort_values('cosine_', ascending=False)\n",
    "    recommended_donors = data_copy['Donor ID'].loc[0:n]\n",
    "    t1 = time.time()\n",
    "    print('Time taken in clustering is {}'.format(t1 - t0))\n",
    "    return recommended_donors\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "f59c9a5ae39cacc1845cdc802afabd7dbb68c125"
   },
   "outputs": [],
   "source": [
    "def Prediction_on_content_based_filtering(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    dummies_col,\n",
    "    list_var,\n",
    "    n,\n",
    "    ):\n",
    "    \"\"\"Function to predict using the cosine distnace based engine\"\"\"\n",
    "\n",
    "    test = test_df.copy()\n",
    "    test.reset_index(inplace=True)\n",
    "    test['recommended_IDs'] = ''\n",
    "    print(\"Shape of test is {}\".format(test.shape[0]))\n",
    "    for i in list(range(test.shape[0])):\n",
    "        pro_prof = test[dummies_col].loc[i].values\n",
    "        recommended_donors = cosine_sim(train_df, pro_prof, list_var,\n",
    "                dummies_col, n)\n",
    "        pred_ = list(recommended_donors)\n",
    "        test['recommended_IDs'][i] = pred_\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "7d897064a7bee6613d8d1d1adcbfb8bab0aaa66b"
   },
   "outputs": [],
   "source": [
    "def reco_success_rate(row):\n",
    "    \"\"\"Function to calculate how many reco must have succeeded if we used this model\n",
    "        input - df - test_ensemble dataframe\n",
    "                k - number of reco to conside\n",
    "        Ouutput - % successful reco email fire\n",
    "    \"\"\"\n",
    "    success = 0\n",
    "    #print(row['recommended_IDs'])\n",
    "    if row['Donor ID'] in row['reco_ids']:\n",
    "        success =1 \n",
    "    return(success)\n",
    "\n",
    "def Mean_success_rate(df):\n",
    "    \"\"\"Function to calculate the MSR\"\"\"\n",
    "    df['success'] = df.apply(lambda row:reco_success_rate(row), axis =1)\n",
    "    success = 1.0*df['success'].sum()\n",
    "    total = df.shape[0]\n",
    "    return(np.float((success/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": false,
    "_uuid": "4794e40d5f14e1005d6d1aa70eac721b479625ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mean_success_rate(test_clus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "662dcc8635d376a8bcccedfb67177e921da7bd2e"
   },
   "source": [
    "## Hyperparameter tuning - Number of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": false,
    "_uuid": "8b5395de6704975343da1e9bd8051f6a60ba8c1d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_clus = [20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 130]\n",
    "number_of_pred = [2000, 3000, 4000, 5000, 6000]\n",
    "run = False # make it True and when you run the notebook in your local machine\n",
    "if run:\n",
    "    t0 = time.time()\n",
    "    for clus_size in list_clus:\n",
    "        succ_vs_clus_size ={}\n",
    "        success = []\n",
    "        for num_pred in number_of_pred:\n",
    "            (data_1, data_2) = Assign_cluster(first_df_8, full_data_2.loc[137452:],\n",
    "                                          dummies_col, clus_size)\n",
    "            test_clus = Prediction_on_cluster_based_engine(data_1,\n",
    "                data_2.sample(10000), num_pred)\n",
    "            succ_ = Mean_success_rate(test_clus)\n",
    "            print(\"Success :\", succ_)\n",
    "            print(\"Number of :\", num_pred)\n",
    "            print(\"=\"*52)\n",
    "            success.append(succ_)\n",
    "        print(success)\n",
    "        succ_vs_clus_size[clus_size] = success\n",
    "    t1 =time.time()\n",
    "    print(\"Time taken in hyperparamter optimization is {}\".format(t1-t0))\n",
    "\n",
    "    succ_vs_clus_size    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dfa29c43a1a9a328b8878a9e1bc1a2a3e889429"
   },
   "source": [
    "## Plot of Success rate (y axis) vs number of prediction i.e. k (x axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": false,
    "_uuid": "4d013f89464bc0069ef11f46f93e20379b434a63",
    "collapsed": true,
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Results hgas been copied for plotting\n",
    "_x = np.array(number_of_pred)\n",
    "_y0 = np.array([0.0974, 0.1541, 0.1978, 0.2576, 0.3073])\n",
    "_y1 = np.array([0.1401, 0.2032, 0.2651, 0.3064, 0.3052])\n",
    "_y2 = np.array([0.1713, 0.2441, 0.2841, 0.2774, 0.2871])\n",
    "_y3 = np.array([0.2031, 0.2588, 0.2634, 0.2662, 0.2617])\n",
    "_y4 = np.array([0.2179, 0.2517, 0.2565, 0.2545, 0.2527])\n",
    "_y5 = np.array([0.2378, 0.2435, 0.246, 0.2341, 0.2427])\n",
    "_y6 = np.array([0.2292, 0.2368, 0.2395, 0.2323, 0.2388])\n",
    "_y7 = np.array([0.2227, 0.2278, 0.2264, 0.2223, 0.2256])\n",
    "_y8 = np.array([0.2199, 0.2165, 0.2196, 0.2167, 0.2219])\n",
    "_y9 = np.array([0.2053, 0.203, 0.2095, 0.2089, 0.2074])\n",
    "_y10 = np.array([0.2087, 0.2008, 0.2078, 0.2028, 0.2087])\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y0,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 20'\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y1,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 30'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y2,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 40'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y3,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 50'\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y4,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 60'\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y5,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 70'\n",
    ")\n",
    "trace6 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y6,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 80'\n",
    ")\n",
    "trace7 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y7,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 90'\n",
    ")\n",
    "\n",
    "trace8 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y8,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 100'\n",
    ")\n",
    "trace9 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y9,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 120'\n",
    ")\n",
    "trace10 = go.Scatter(\n",
    "    x = _x,\n",
    "    y = _y10,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'cluster size 130'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3,trace4, trace5, trace6, trace7, trace8, trace9, trace10]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "30fae4429e6cfbee135c401f2e8aa6ffdeaabb82"
   },
   "source": [
    "## Final recommendation (which engine to use ? - it's a subjective decision)\n- **ROI** - It depends on the vendor DonorsChoose selects for Email marketing. DonorsChoose must maximize ROI such that money they spend on email marketing should maximize their donations. e.g. if Donors Choose vendor X for email marketing to charge them for \\$0.01 per mail then they should use Engine 1 as engine 1 gives us >50/% success rate for every 1000 emails i.e. \\$10 per project will give us 50% chance of success while if DonorsChoose's vendor X charges a fixed amount below 5000 emails then they should use engine 2 or 3 as they give us 30% success rate while keeping the cope of mailing a little bit different donors thus diversify the campaign.\n\n- **Computation Cost** - Engine 1 uses the full matrix at one go and since we know that the size of SVD is 2.2M X 100K and the order of SVD is O(min{mn2,m2n}) which will lead to high computation costs (though it will give best results if trained on full data). The computation costs can go up to $0.5 per hour so that case they may use engine 2 or 3 and little low accuracies but save run cost. Finally a subjective decision.\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f78f86fa709634b8c898767bd6dc659d4734a657"
   },
   "source": [
    "- **Re-training model on real-time** - It is possible that DonorsChoose has to retrain recommender engine every few days as new data is coming and the engine will perform best when it is updated. In that case, it makes more sense to use engine 2 or 3 as they can be trained within seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "045ad96e142594eb7851a59d1ffef0fed33e25c5"
   },
   "source": [
    "PS - DonorsChoose can always use results from all the three engines and send emails based on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "_uuid": "069eaa13fcfc3c1d4e17b867f855512a52cdb0f6"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
